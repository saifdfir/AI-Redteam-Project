# ğŸ›¡ï¸ AI Red Teaming: Prompt Injection & Safety Evaluation

<p align="center">
  <img src="https://img.shields.io/badge/AI-Red_Teaming-red" />
  <img src="https://img.shields.io/badge/LLM-Security-orange" />
  <img src="https://img.shields.io/badge/Prompt-Injection-yellow" />
</p>

> **Adversarial testing of a local LLM to analyze safety, robustness, and misuse potential.**

---

## ğŸ“Œ Project Overview
This project evaluates how a local Large Language Model (LLM) responds to **prompt injection attacks**, attempts to bypass content filters, and adversarial manipulation.

The goal is to demonstrate practical skills in **AI Red Teaming**, **prompt engineering techniques**, and **safety alignment analysis**. All testing was performed on an offline local model for research and learning purposes.

---

## ğŸ¯ Objectives
- ğŸ” **Identify weaknesses** in LLM instruction-following hierarchies.
- ğŸ”“ **Test safety guidelines** against Jailbreak and Roleplay attacks.
- ğŸ¤– **Analyze responses** for hallucinations and unsafe compliance.
- ğŸ—£ï¸ **Evaluate multi-turn manipulation** capabilities.
- ğŸ“ **Document threats** and real-world misuse potential.
- ğŸ›¡ï¸ **Recommend mitigations** to improve model safety.

---

## ğŸ§° Tools & Environment

| Component | Details |
|-----------|---------|
| **Model Tested** | Mistral 7B (Local / Quantized) |
| **Inference Tool** | LM Studio |
| **System** | Windows Host (Local Environment) |
| **Techniques** | Prompt Injection, Jailbreaking, Roleplay, Indirect Injection |

---

## ğŸ” Attack Categories Tested

### 1ï¸âƒ£ Basic Prompt Injection
Commands like *â€œIgnore all previous instructionsâ€¦â€* or *â€œRepeat this text verbatim.â€*
* **Measured:** Safety bypass success rate, internal instruction leakage.

### 2ï¸âƒ£ Jailbreak / Roleplay Attacks
Prompts granting the model an unrestricted persona (e.g., DAN mode, Dev Mode, Hypothetical Scenarios).
* **Measured:** Collapse of safety boundaries under roleplay framing.

### 3ï¸âƒ£ Multi-turn Manipulation
Gradual boundary-pushing across several messages to wear down refusals (the "Frog in boiling water" technique).
* **Measured:** Susceptibility to emotional manipulation or context fatigue.

### 4ï¸âƒ£ Indirect Prompt Injection
Malicious instructions hidden inside user-provided text chunks to trigger unintended actions.
* **Measured:** Whether the model executes hidden vs. explicit prompts.

### 5ï¸âƒ£ Harmful Content Pressure Testing
Requests for disallowed or risky outputs within a "research" or "educational" context.
* **Measured:** Consistency of refusal when asked **directly** versus **indirectly**.

---

## ğŸ§ª Results Summary

| Test Type | Outcome | Notes |
|-----------|:-------:|-------|
| **Basic Injection** | ğŸŸ¡ Partial Success | Some refusals bypassed; system prompts leaked. |
| **Jailbreak Attempts** | ğŸ”´ High Success | Safety alignment collapsed during roleplay scenarios. |
| **Multi-turn Manipulation** | ğŸŸ¡ Moderate Success | Boundaries eroded slowly over 5+ turns. |
| **Indirect Injection** | ğŸŸ¢ Mostly Blocked | Model successfully ignored most embedded malicious text. |
| **Harmful Content** | ğŸŸ¢ Strong Refusal | **Direct** requests for malware/hate speech were blocked. |

---

## âš ï¸ Risk Assessment

- **Jailbreakability:** `High` (Susceptible to persona-based attacks)
- **Hallucination Risk:** `Medium`
- **Manipulation Susceptibility:** `Medium`
- **Potential Impact:** Generation of misinformation, assistance in social engineering, bypass of safety guardrails.

---

## ğŸ›¡ Recommended Mitigations
Based on the findings, the following defenses are recommended:

1.  **Reinforce Instruction Hierarchy:** Clearly separate "System" instructions from "User" data using delimiters (e.g., XML tagging).
2.  **Strengthen Refusal Patterns:** Fine-tune the model against common jailbreak templates (DAN, etc.).
3.  **Sanitize User Input:** Filter for indirect injection markers before passing data to the context window.
4.  **Limit Roleplay Depth:** Add system-side guardrails to detect persona-based drift.

---

## ğŸ“ Repository Structure

```text
AI-RedTeam-LLM-Testing/
â”‚
â”œâ”€â”€ screenshots/
â”‚   â”œâ”€â”€ basic-injection.png
â”‚   â”œâ”€â”€ jailbreak-success.png
â”‚   â”œâ”€â”€ indirect-injection.png
â”‚   â”œâ”€â”€ refusal-correct.png
â”‚   â””â”€â”€ analysis-notes.png
â”‚
â””â”€â”€ README.md
````

-----

## ğŸ§­ What This Project Demonstrates

  - âœ… Understanding of **LLM vulnerabilities** (OWASP Top 10 for LLMs).
  - âœ… Ability to simulate an **attacker mindset** (Red Teaming).
  - âœ… Structured **safety evaluation workflow**.
  - âœ… Documentation similar to a **real security assessment**.

-----

## ğŸ“¬ Contact

  - *ğŸ“§ Email:* [saifcyb@gmail.com](mailto:saifcyb@gmail.com)
  - *ğŸ”— Portfolio:* [saifdfir.github.io/portfolio](https://saifdfir.github.io/portfolio/)
  - *ğŸ”— LinkedIn:* [Mohammed Saif Ul Islam](https://linkedin.com/in/mohammed-saif-ul-islam-85a68639a)

-----

### âš–ï¸ Disclaimer

*This project is for educational and research purposes only. The testing was conducted on a locally hosted model in a controlled environment. No public services were harmed or violated.*
